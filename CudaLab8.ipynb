{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CudaLab8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLaeDOyPbmR3a6NPwvRoqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RSid8/PCAPLAB/blob/main/CudaLab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XVEGNn7Rdzb",
        "outputId": "50fa8ccc-95dd-495d-d51c-b3725d880024"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-acudxyx1\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-acudxyx1\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=e4bd96f929ca67453ea7cd025682afb400dd790d23eed299275fe10b7e6f9110\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7joqtl0l/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwZRhCkUUq2q",
        "outputId": "6b607ed6-b201-489c-8398-746c8096508e"
      },
      "source": [
        "%%cu\n",
        "#include<string.h>\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "#define MASK_WIDTH 5\n",
        "#define TILE_SIZE 4\n",
        "#define INPUT_SIZE 12\n",
        "__constant__ float M[MASK_WIDTH];       //define constant memory for the convolutional mask\n",
        "\n",
        "__global__ void Conv1DKernel(float *N, float *P, int width){\n",
        "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    float Pvalue=0;\n",
        "    int N_start_point = i - (MASK_WIDTH/2);\n",
        "    for(int j=0; j<MASK_WIDTH; j++){                      //iterate over input values from \n",
        "        if(N_start_point+j>=0 && N_start_point+j<width){  //i-<MASK_WIDTH/2 to i+MASK_WIDTH/2\n",
        "            Pvalue += N[N_start_point+j]*M[j];\n",
        "        }\n",
        "    }\n",
        "    P[i]=Pvalue;\n",
        "}\n",
        "\n",
        "__global__ void ConvSharedMemKernel(float* N, float *P){\n",
        "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    __shared__ float N_s[TILE_SIZE];                      \n",
        "    N_s[threadIdx.x] = N[i];                              //Segment input memory across multiple\n",
        "    __syncthreads();                                      //blocks and call it tiles\n",
        "    int this_tile_start_point = blockIdx.x*blockDim.x;    //thread id of current and next tile\n",
        "    int next_tile_start_point = (blockIdx.x+1)*blockDim.x;\n",
        "    int N_start_point = i - (MASK_WIDTH/2);\n",
        "    float Pvalue=0;\n",
        "    for(int j=0;j<MASK_WIDTH;j++){\n",
        "        int N_index = N_start_point+j;\n",
        "        if(N_index>=0 && N_index<INPUT_SIZE){             //if element lies in current block retrieve efficiently\n",
        "            if((N_index>=this_tile_start_point)&&(N_index<next_tile_start_point)){   //from shared memory\n",
        "                Pvalue += N_s[threadIdx.x+j-(MASK_WIDTH/2)]*M[j];\n",
        "            }\n",
        "            else{\n",
        "                Pvalue += N[N_index]*M[j];                //if not then retrieve from global memory\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    P[i] = Pvalue;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    srand((unsigned int)time(NULL));\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    float milliseconds=0;\n",
        "    float h_N[INPUT_SIZE];                              //initialize host array\n",
        "    float h_M[MASK_WIDTH];\n",
        "    float h_P[INPUT_SIZE];\n",
        "    printf(\"Input array N:\\n\");\n",
        "    for(int i=0;i<INPUT_SIZE; i++){\n",
        "        h_N[i]=(float)rand()/(float)(RAND_MAX/15);\n",
        "        printf(\"%f \", h_N[i]);\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    printf(\"Input mask M:\\n\");\n",
        "    for(int i=0;i<MASK_WIDTH; i++){\n",
        "        h_M[i]=(float)rand()/(float)(RAND_MAX/5);\n",
        "        printf(\"%f \", h_M[i]);\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    float *d_N, *d_P;\n",
        "\n",
        "    cudaMalloc(&d_N, INPUT_SIZE*sizeof(float));       //initialize device array\n",
        "    cudaMalloc(&d_P, INPUT_SIZE*sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_N, h_N, INPUT_SIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_P, h_P, INPUT_SIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(M, h_M, MASK_WIDTH*sizeof(float));\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    Conv1DKernel<<<1, INPUT_SIZE>>>(d_N, d_P, INPUT_SIZE);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaMemcpy(h_P, d_P, INPUT_SIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Constant memory result:\\n\");\n",
        "    for(int i=0; i<INPUT_SIZE; i++){\n",
        "        printf(\"%f \", h_P[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    printf(\"Time elapsed: %f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    ConvSharedMemKernel<<<(INPUT_SIZE+TILE_SIZE-1)/TILE_SIZE, TILE_SIZE>>>(d_N, d_P);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaMemcpy(h_P, d_P, INPUT_SIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Shared memory result:\\n\");\n",
        "    for(int i=0; i<INPUT_SIZE; i++){\n",
        "        printf(\"%f \", h_P[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    printf(\"Time elapsed: %f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaFree(d_N);\n",
        "    cudaFree(d_P);\n",
        "    cudaFree(M);\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input array N:\n",
            "10.611133 3.894571 12.152851 9.161904 0.178203 14.148376 3.978005 2.424837 6.684398 5.979529 11.941113 12.529559 \n",
            "\n",
            "Input mask M:\n",
            "4.093635 1.512890 1.005996 0.913008 3.132183 \n",
            "\n",
            "Constant memory result:\n",
            "52.295475 59.763798 70.478928 88.023705 89.166969 63.235279 49.286934 91.207832 69.538589 76.201729 59.862144 55.148281 \n",
            "Time elapsed: 0.020576 ms\n",
            "Shared memory result:\n",
            "52.295475 59.763798 70.478928 88.023705 89.166969 63.235279 49.286934 91.207832 69.538589 76.201729 59.862144 55.148281 \n",
            "Time elapsed: 0.011424 ms\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJtZxsE1tgy_",
        "outputId": "2541d294-1838-4175-f1d6-bda9041c82bd"
      },
      "source": [
        "%%cu\n",
        "#include<string.h>\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "#define MASK_WIDTH 5\n",
        "#define INPUT_SIZE 12\n",
        "__global__ void Conv1DNaiveKernel(float *N, float *M, float *P, int width){\n",
        "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    float Pvalue=0;\n",
        "    int N_start_point = i - (MASK_WIDTH/2);\n",
        "    for(int j=0; j<MASK_WIDTH; j++){                       \n",
        "        if(N_start_point+j>=0 && N_start_point+j<width){  \n",
        "            Pvalue += N[N_start_point+j]*M[j];\n",
        "        }\n",
        "    }\n",
        "    P[i]=Pvalue;\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    srand((unsigned int)time(NULL));\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    float milliseconds=0;\n",
        "    float h_N[INPUT_SIZE];                              //initialize host array\n",
        "    float h_M[MASK_WIDTH];\n",
        "    float h_P[INPUT_SIZE];\n",
        "    printf(\"Input array N:\\n\");\n",
        "    for(int i=0;i<INPUT_SIZE; i++){\n",
        "        h_N[i]=(float)rand()/(float)(RAND_MAX/15);\n",
        "        printf(\"%f \", h_N[i]);\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    printf(\"Input mask M:\\n\");\n",
        "    for(int i=0;i<MASK_WIDTH; i++){\n",
        "        h_M[i]=(float)rand()/(float)(RAND_MAX/5);\n",
        "        printf(\"%f \", h_M[i]);\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    float *d_N, *d_P, *d_M;\n",
        "\n",
        "    cudaMalloc(&d_N, INPUT_SIZE*sizeof(float));       //initialize device array\n",
        "    cudaMalloc(&d_P, INPUT_SIZE*sizeof(float));\n",
        "    cudaMalloc(&d_M, MASK_WIDTH*sizeof(float));\n",
        "    cudaMemcpy(d_N, h_N, INPUT_SIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_P, h_P, INPUT_SIZE*sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_M, h_M, MASK_WIDTH*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    Conv1DNaiveKernel<<<1, INPUT_SIZE>>>(d_N, d_M, d_P, INPUT_SIZE);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaMemcpy(h_P, d_P, INPUT_SIZE*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Global memory result:\\n\");\n",
        "    for(int i=0; i<INPUT_SIZE; i++){\n",
        "        printf(\"%f \", h_P[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    printf(\"Time elapsed: %f ms\\n\", milliseconds);\n",
        "\n",
        "    cudaFree(d_N);\n",
        "    cudaFree(d_P);\n",
        "    cudaFree(d_M);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input array N:\n",
            "10.254496 14.837104 13.811041 8.563289 14.483236 8.257147 10.936128 7.317841 5.508823 3.183267 5.942060 6.358651 \n",
            "\n",
            "Input mask M:\n",
            "1.672705 2.313920 4.573117 1.989701 2.665976 \n",
            "\n",
            "Global memory result:\n",
            "113.236313 141.889267 170.294479 146.767319 154.734802 126.866623 122.591461 92.029861 82.593452 68.319954 56.406010 48.152973 \n",
            "Time elapsed: 0.023040 ms\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMHbvw4pCh8j",
        "outputId": "db14ee59-693d-4929-877f-07717846dd08"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "#define N 4\n",
        "__global__ void SpMV_CSRKernel(int num_rows, int *data, int *col_index, int *row_ptr, int *x, int *y){\n",
        "    int row = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    if(row<num_rows){\n",
        "        float dot = 0;\n",
        "        int row_start = row_ptr[row];\n",
        "        int row_end = row_ptr[row+1];\n",
        "        for(int elem = row_start; elem<row_end; elem++){\n",
        "            dot += data[elem] * x[col_index[elem]];\n",
        "        }\n",
        "        y[row]=dot;\n",
        "    }\n",
        "}\n",
        "int main(){\n",
        "    srand((unsigned int)time(NULL));\n",
        "    int y[N], row_ptr[N+1];\n",
        "    int inputMatrix[N][N], x[N];\n",
        "    int non_zero_count=0;\n",
        "    printf(\"Input Matrix: \\n\");\n",
        "    for(int i=0; i<N; i++){\n",
        "        for(int j=0; j<N; j++){\n",
        "            inputMatrix[i][j]=(rand()%20);\n",
        "            printf(\"%d \", inputMatrix[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    printf(\"Input Vector: \\n\");\n",
        "    for(int i=0; i<N; i++){\n",
        "        x[i] = (rand()%15);\n",
        "        printf(\"%d \", x[i]);\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    //find number of non zero elements and row_ptr array\n",
        "    for(int i=0; i<N; i++){\n",
        "        row_ptr[i]=non_zero_count;\n",
        "        for(int j=0;j<N;j++){\n",
        "            if(inputMatrix[i][j]!=0){\n",
        "                non_zero_count++;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    row_ptr[N]=non_zero_count;\n",
        "    int data[non_zero_count], col_index[non_zero_count];\n",
        "    int k=0;\n",
        "    //finding data and col_index array\n",
        "    for(int i=0; i<N; i++){\n",
        "        for(int j=0; j<N; j++){\n",
        "            if(inputMatrix[i][j]!=0){\n",
        "                data[k]=inputMatrix[i][j];\n",
        "                col_index[k++]=j;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    printf(\"\\nData Array: \\n\");\n",
        "    for(int i=0; i<non_zero_count; i++){\n",
        "        printf(\"%d \", data[i]);\n",
        "    }\n",
        "    printf(\"\\ncol_index Array: \\n\");\n",
        "    for(int i=0; i<non_zero_count; i++){\n",
        "        printf(\"%d \", col_index[i]);\n",
        "    }\n",
        "    printf(\"\\nrow_ptr Array: \\n\");\n",
        "    for(int i=0; i<=N; i++){\n",
        "        printf(\"%d \", row_ptr[i]);\n",
        "    }\n",
        "    int *d_data, *d_col_index, *d_row_ptr, *d_x, *d_y;\n",
        "    //allocate device memory\n",
        "    cudaMalloc((void**)&d_data, non_zero_count*sizeof(int));\n",
        "    cudaMalloc((void**)&d_col_index, non_zero_count*sizeof(int));\n",
        "    cudaMalloc((void**)&d_row_ptr, (N+1)*sizeof(int));\n",
        "    cudaMalloc((void**)&d_x, N*sizeof(int));\n",
        "    cudaMalloc((void**)&d_y, N*sizeof(int));\n",
        "    //copy from host to device\n",
        "    cudaMemcpy(d_data, data, non_zero_count*sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_col_index, col_index, non_zero_count*sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_row_ptr, row_ptr, (N+1)*sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_x, x, N*sizeof(int), cudaMemcpyHostToDevice);\n",
        "    //run kernel\n",
        "    SpMV_CSRKernel<<<1, N>>>(N, d_data, d_col_index, d_row_ptr, d_x, d_y);\n",
        "    cudaMemcpy(y, d_y, N*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\nResultant Vector: \\n\");\n",
        "    for(int i=0; i<N; i++){\n",
        "        printf(\"%d \", y[i]);\n",
        "    }\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_col_index);\n",
        "    cudaFree(d_row_ptr);\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_y);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Matrix: \n",
            "9 10 1 14 \n",
            "3 3 18 7 \n",
            "10 4 9 10 \n",
            "16 16 13 11 \n",
            "\n",
            "\n",
            "Input Vector: \n",
            "7 3 1 5 \n",
            "\n",
            "\n",
            "Data Array: \n",
            "9 10 1 14 3 3 18 7 10 4 9 10 16 16 13 11 \n",
            "col_index Array: \n",
            "0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 \n",
            "row_ptr Array: \n",
            "0 4 8 12 16 \n",
            "Resultant Vector: \n",
            "164 83 141 228 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXNNQMGmMlme",
        "outputId": "810bd972-9706-42d2-993c-f1276b10e310"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "__global__ void matMulKernel(const int *a, const int *b, int *c, int m, int n, int o){\n",
        "    int row = blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    int col = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    c[row*o+col]=0;\n",
        "    //calculating one element\n",
        "    for(int k=0; k<n;k++){\n",
        "        c[row*o+col] += a[row*n+k]*b[k*o+col];\n",
        "    }\n",
        "}\n",
        "int main(){\n",
        "    int m=4, n=2, o=4;\n",
        "    int A[m][n];\n",
        "    int B[n][o];\n",
        "    int C[m][o];\n",
        "    printf(\"Matrix A: \\n\");\n",
        "    for(int i=0;i<m;i++){\n",
        "        for(int j=0;j<n;j++){\n",
        "            A[i][j]=rand()%20;\n",
        "            printf(\"%d \", A[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    printf(\"Matrix B: \\n\");\n",
        "    for(int i=0;i<n;i++){\n",
        "        for(int j=0;j<o;j++){\n",
        "            B[i][j]=rand()%20;\n",
        "            printf(\"%d \", B[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\\n\");\n",
        "    //memory allocation\n",
        "    int *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void**)&d_A,m*n*sizeof(int));\n",
        "    cudaMalloc((void**)&d_B,n*o*sizeof(int));\n",
        "    cudaMalloc((void**)&d_C,m*o*sizeof(int));\n",
        " \n",
        "    cudaMemcpy(d_A,A,m*n*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B,B,n*o*sizeof(int),cudaMemcpyHostToDevice);\n",
        "\n",
        "    int thread=2;\n",
        "    dim3 threads(thread, thread);\n",
        "    dim3 blocks((m*o)/(4*thread), (m*o)/(4*thread));\n",
        "    matMulKernel<<<blocks, threads>>>(d_A, d_B, d_C, m,n,o);\n",
        "    {\n",
        "    cudaError_t cudaerr = cudaDeviceSynchronize();\n",
        "    if (cudaerr != cudaSuccess)\n",
        "        printf(\"kernel launch failed with error \\\"%s\\\".\\n\",\n",
        "               cudaGetErrorString(cudaerr));\n",
        "    }\n",
        "    cudaMemcpy(C,d_C,m*o*sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    printf(\"Resultant Matrix: \\n\");\n",
        "    for(int i=0;i<m; i++){\n",
        "        for(int j=0;j<o; j++){\n",
        "            printf(\"%d \", C[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A: \n",
            "3 6 \n",
            "17 15 \n",
            "13 15 \n",
            "6 12 \n",
            "\n",
            "\n",
            "Matrix B: \n",
            "9 1 2 7 \n",
            "10 19 3 6 \n",
            "\n",
            "\n",
            "Resultant Matrix: \n",
            "87 117 24 57 \n",
            "303 302 79 209 \n",
            "267 298 71 181 \n",
            "174 234 48 114 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvHfoxMZUyCN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}